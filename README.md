# torch-BlazefaceDistillation


# Abstract
This is Distillation for Converting performance of [MobileNet backborn BlazeFace(google)](https://google.github.io/mediapipe/solutions/face_detection.html) to ResNet backborn BlazeFace(original).

Use KL divergence loss and softmax with temperature.

<img src="https://user-images.githubusercontent.com/48679574/122850849-e3ed0700-d348-11eb-9fb9-ac0a18dc8b1d.jpeg" width="700px">





# Distillation perfomance (Resnet backborn BlazeFace)

<b>1.MobileNet-backborn.(left)   Resnet-backborn(right)</b>





<b>2.MobileNet-backborn.(left)   Resnet-backborn(right)</b>


# training log 


