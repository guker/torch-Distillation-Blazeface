# torch-BlazefaceDistillation


# Abstract
This is Distillation for Converting performance of [MobileNet backborn BlazeFace(google)](https://google.github.io/mediapipe/solutions/face_detection.html) to ResNet backborn BlazeFace(original).

Use KL divergence loss and softmax with temperature.



# Distillation perfomance (Resnet backborn BlazeFace)

<b>1.MobileNet-backborn.(left)   Resnet-backborn(right)</b>





<b>2.MobileNet-backborn.(left)   Resnet-backborn(right)</b>


# training log 


